{
  "timestamp": "2025-09-26T15:45:17.151415",
  "base_url": "http://localhost:18011",
  "summary": {
    "working_endpoints": 5,
    "missing_endpoints": 13,
    "version_info": {}
  },
  "detailed_results": [
    {
      "endpoint": "/health",
      "description": "Basic health check",
      "url": "http://localhost:18011/health",
      "status_code": 200,
      "status_text": "OK",
      "success": true,
      "headers": {
        "date": "Fri, 26 Sep 2025 14:44:38 GMT",
        "server": "uvicorn",
        "content-length": "248",
        "content-type": "application/json"
      },
      "response_size": 248,
      "json_response": {
        "status": "healthy",
        "api_version": "1.2.0",
        "videoannotator_version": "1.2.2",
        "message": "VideoAnnotator API is running",
        "logging": "enhanced",
        "memory_percent": 57.5,
        "database": {
          "status": "healthy",
          "message": "Database healthy - 2 jobs in sqlite backend"
        }
      }
    },
    {
      "endpoint": "/api/v1/system/health",
      "description": "Detailed system health",
      "url": "http://localhost:18011/api/v1/system/health",
      "status_code": 200,
      "status_text": "OK",
      "success": true,
      "headers": {
        "date": "Fri, 26 Sep 2025 14:44:41 GMT",
        "server": "uvicorn",
        "content-length": "1049",
        "content-type": "application/json"
      },
      "response_size": 1049,
      "json_response": {
        "status": "healthy",
        "timestamp": "2025-09-26T14:44:42.512520",
        "api_version": "1.2.0",
        "videoannotator_version": "1.2.2",
        "system": {
          "platform": "Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35",
          "python_version": "3.12.11",
          "cpu_count": 24,
          "cpu_percent": 0.4,
          "memory_percent": 57.5,
          "memory": {
            "total": 16642498560,
            "available": 7079280640,
            "percent": 57.5,
            "used": 9217142784,
            "free": 5601054720
          },
          "disk": {
            "total": 1081101176832,
            "used": 277774360576,
            "free": 748334460928,
            "percent": 25.69365074506485
          }
        },
        "database": {
          "status": "healthy",
          "message": "Database healthy - 2 jobs in sqlite backend"
        },
        "gpu": {
          "available": true,
          "device_count": 1,
          "current_device": 0,
          "device_name": "NVIDIA GeForce GTX 1060 6GB",
          "memory_allocated": 0,
          "memory_reserved": 0
        },
        "pipelines": {
          "total": 6,
          "names": [
            "audio_processing",
            "face_laion_clip",
            "face_openface3_embedding",
            "person_tracking",
            "scene_detection",
            "voice_emotion_baseline"
          ]
        },
        "services": {
          "database": {
            "status": "healthy",
            "message": "Database healthy - 2 jobs in sqlite backend"
          },
          "job_queue": "embedded",
          "pipelines": "ready"
        },
        "uptime_seconds": 14029
      }
    },
    {
      "endpoint": "/api/v1/system/info",
      "description": "System information (v1.2.x)",
      "url": "http://localhost:18011/api/v1/system/info",
      "status_code": 404,
      "status_text": "Not Found",
      "success": false,
      "headers": {
        "date": "Fri, 26 Sep 2025 14:44:44 GMT",
        "server": "uvicorn",
        "content-length": "22",
        "content-type": "application/json"
      },
      "response_size": 22,
      "json_response": {
        "detail": "Not Found"
      }
    },
    {
      "endpoint": "/api/v1/system/server-info",
      "description": "Server information (v1.2.x)",
      "url": "http://localhost:18011/api/v1/system/server-info",
      "status_code": 404,
      "status_text": "Not Found",
      "success": false,
      "headers": {
        "date": "Fri, 26 Sep 2025 14:44:46 GMT",
        "server": "uvicorn",
        "content-length": "22",
        "content-type": "application/json"
      },
      "response_size": 22,
      "json_response": {
        "detail": "Not Found"
      }
    },
    {
      "endpoint": "/api/v1/system/version",
      "description": "Version endpoint (v1.2.x)",
      "url": "http://localhost:18011/api/v1/system/version",
      "status_code": 404,
      "status_text": "Not Found",
      "success": false,
      "headers": {
        "date": "Fri, 26 Sep 2025 14:44:48 GMT",
        "server": "uvicorn",
        "content-length": "22",
        "content-type": "application/json"
      },
      "response_size": 22,
      "json_response": {
        "detail": "Not Found"
      }
    },
    {
      "endpoint": "/api/v1/system/capabilities",
      "description": "Server capabilities (v1.2.x)",
      "url": "http://localhost:18011/api/v1/system/capabilities",
      "status_code": 404,
      "status_text": "Not Found",
      "success": false,
      "headers": {
        "date": "Fri, 26 Sep 2025 14:44:51 GMT",
        "server": "uvicorn",
        "content-length": "22",
        "content-type": "application/json"
      },
      "response_size": 22,
      "json_response": {
        "detail": "Not Found"
      }
    },
    {
      "endpoint": "/api/v1/pipelines",
      "description": "Legacy pipeline list",
      "url": "http://localhost:18011/api/v1/pipelines",
      "status_code": 200,
      "status_text": "OK",
      "success": true,
      "headers": {
        "date": "Fri, 26 Sep 2025 14:44:53 GMT",
        "server": "uvicorn",
        "content-length": "6236",
        "content-type": "application/json"
      },
      "response_size": 6236,
      "json_response": {
        "pipelines": [
          {
            "name": "audio_processing",
            "display_name": "Audio Processing (Speech + Diarization)",
            "description": "Transcribe speech with Whisper and perform speaker diarization using pyannote.audio producing WebVTT + RTTM outputs.",
            "enabled": true,
            "pipeline_family": "audio",
            "variant": "whisper-pyannote",
            "tasks": [
              "speech-transcription",
              "speaker-diarization"
            ],
            "modalities": [
              "audio"
            ],
            "capabilities": [
              "streaming",
              "embedding"
            ],
            "backends": [
              "pytorch"
            ],
            "stability": "beta",
            "outputs": [
              {
                "format": "WebVTT",
                "types": [
                  "transcript"
                ]
              },
              {
                "format": "RTTM",
                "types": [
                  "speaker_turns"
                ]
              }
            ],
            "config_schema": {
              "whisper_model": {
                "type": "string",
                "default": "base",
                "description": "Whisper model size"
              },
              "enable_diarization": {
                "type": "boolean",
                "default": true,
                "description": "Perform speaker diarization"
              },
              "min_speakers": {
                "type": "integer",
                "default": 1,
                "description": "Minimum number of speakers to consider"
              },
              "max_speakers": {
                "type": "integer",
                "default": 10,
                "description": "Maximum number of speakers to consider"
              }
            },
            "examples": [
              {
                "cli": "videoannotator job submit demo.mp4 --pipelines audio_processing",
                "description": "Submit audio processing job"
              }
            ]
          },
          {
            "name": "face_laion_clip",
            "display_name": "LAION CLIP Face Semantic Embedding",
            "description": "Produce semantic face embeddings using a CLIP-derived model; supports zero-shot attribute and emotion tagging.",
            "enabled": true,
            "pipeline_family": "face",
            "variant": "laion-clip-face",
            "tasks": [
              "face-embedding",
              "face-recognition",
              "emotion-recognition"
            ],
            "modalities": [
              "image",
              "video"
            ],
            "capabilities": [
              "zero-shot",
              "embedding",
              "real-time"
            ],
            "backends": [
              "pytorch"
            ],
            "stability": "experimental",
            "outputs": [
              {
                "format": "JSON",
                "types": [
                  "embeddings",
                  "attributes"
                ]
              }
            ],
            "config_schema": {
              "model_name": {
                "type": "string",
                "default": "laion/CLIP-ViT-B-32",
                "description": "CLIP model identifier"
              },
              "embedding_dim": {
                "type": "integer",
                "default": 512,
                "description": "Output embedding dimensionality"
              },
              "enable_emotion": {
                "type": "boolean",
                "default": true,
                "description": "Enable emotion category projection"
              }
            },
            "examples": [
              {
                "cli": "videoannotator job submit demo.mp4 --pipelines face_laion_clip",
                "description": "Submit LAION CLIP face embedding job"
              }
            ]
          },
          {
            "name": "face_openface3_embedding",
            "display_name": "OpenFace3 Face Embedding",
            "description": "Generate 512-D face embeddings for recognition or clustering using OpenFace3.",
            "enabled": true,
            "pipeline_family": "face",
            "variant": "openface3-embedding",
            "tasks": [
              "face-embedding"
            ],
            "modalities": [
              "image",
              "video"
            ],
            "capabilities": [
              "embedding"
            ],
            "backends": [
              "onnx",
              "pytorch"
            ],
            "stability": "experimental",
            "outputs": [
              {
                "format": "JSON",
                "types": [
                  "embeddings"
                ]
              }
            ],
            "config_schema": {
              "model_path": {
                "type": "string",
                "default": "models/face/openface3.onnx",
                "description": "Path to OpenFace3 ONNX model"
              },
              "batch_size": {
                "type": "integer",
                "default": 32,
                "description": "Inference batch size"
              },
              "min_face_size": {
                "type": "integer",
                "default": 32,
                "description": "Minimum face size in pixels to consider"
              }
            },
            "examples": [
              {
                "cli": "videoannotator job submit demo.mp4 --pipelines face_openface3_embedding",
                "description": "Submit face embedding job"
              }
            ]
          },
          {
            "name": "person_tracking",
            "display_name": "Person Tracking & Pose",
            "description": "Track persons with YOLO11 + ByteTrack producing COCO person + keypoint annotations.",
            "enabled": true,
            "pipeline_family": "person",
            "variant": "yolov11n-pose-bytetrack",
            "tasks": [
              "object-tracking",
              "pose-estimation"
            ],
            "modalities": [
              "video"
            ],
            "capabilities": [
              "real-time",
              "identity-persistence"
            ],
            "backends": [
              "pytorch"
            ],
            "stability": "beta",
            "outputs": [
              {
                "format": "COCO",
                "types": [
                  "person_detection",
                  "keypoints",
                  "tracking"
                ]
              }
            ],
            "config_schema": {
              "model": {
                "type": "string",
                "default": "models/yolo/yolo11n-pose.pt",
                "description": "YOLO pose model path or tag"
              },
              "conf_threshold": {
                "type": "float",
                "default": 0.4,
                "description": "Detection confidence threshold"
              },
              "iou_threshold": {
                "type": "float",
                "default": 0.7,
                "description": "IoU threshold for NMS/tracking"
              },
              "track_mode": {
                "type": "boolean",
                "default": true,
                "description": "Enable tracking logic"
              }
            },
            "examples": [
              {
                "cli": "videoannotator job submit demo.mp4 --pipelines person_tracking",
                "description": "Submit person tracking job"
              },
              {
                "api": "POST /api/v1/jobs (selected_pipelines=[\"person_tracking\"])",
                "description": "API job submission including person tracking"
              }
            ]
          },
          {
            "name": "scene_detection",
            "display_name": "Scene Detection",
            "description": "Detect hard cuts and scene boundaries using PySceneDetect with semantic labeling via CLIP.",
            "enabled": true,
            "pipeline_family": "scene",
            "variant": "pyscenedetect-clip",
            "tasks": [
              "scene-detection",
              "scene-segmentation"
            ],
            "modalities": [
              "video"
            ],
            "capabilities": [
              "batch",
              "embedding"
            ],
            "backends": [
              "pytorch"
            ],
            "stability": "beta",
            "outputs": [
              {
                "format": "JSON",
                "types": [
                  "scene_boundary",
                  "scene_category"
                ]
              }
            ],
            "config_schema": {
              "threshold": {
                "type": "float",
                "default": 30.0,
                "description": "Scene cut detection threshold"
              },
              "min_scene_length": {
                "type": "float",
                "default": 1.0,
                "description": "Minimum scene length in seconds"
              },
              "enable_clip_labels": {
                "type": "boolean",
                "default": true,
                "description": "Use CLIP embeddings for environment labels"
              }
            },
            "examples": [
              {
                "cli": "videoannotator job submit demo.mp4 --pipelines scene_detection",
                "description": "Submit scene detection job"
              }
            ]
          },
          {
            "name": "voice_emotion_baseline",
            "display_name": "Voice Emotion + Transcription (Baseline)",
            "description": "Performs speech transcription and basic vocal emotion recognition using a lightweight spectral CNN classifier over Whisper embeddings.",
            "enabled": true,
            "pipeline_family": "audio",
            "variant": "whisper-spectral-emotion",
            "tasks": [
              "speech-transcription",
              "emotion-recognition"
            ],
            "modalities": [
              "audio"
            ],
            "capabilities": [
              "streaming",
              "embedding"
            ],
            "backends": [
              "pytorch"
            ],
            "stability": "experimental",
            "outputs": [
              {
                "format": "WebVTT",
                "types": [
                  "transcript"
                ]
              },
              {
                "format": "JSON",
                "types": [
                  "emotion_segments"
                ]
              }
            ],
            "config_schema": {
              "whisper_model": {
                "type": "string",
                "default": "base",
                "description": "Whisper model size"
              },
              "emotion_window": {
                "type": "float",
                "default": 2.0,
                "description": "Sliding window size (seconds) for emotion inference"
              },
              "emotion_stride": {
                "type": "float",
                "default": 0.5,
                "description": "Step size (seconds) between emotion windows"
              },
              "enable_transcription": {
                "type": "boolean",
                "default": true,
                "description": "Toggle transcription"
              }
            },
            "examples": [
              {
                "cli": "videoannotator job submit demo.mp4 --pipelines voice_emotion_baseline",
                "description": "Submit voice transcription + emotion job"
              }
            ]
          }
        ],
        "total": 6
      }
    },
    {
      "endpoint": "/api/v1/pipelines/catalog",
      "description": "Pipeline catalog (v1.2.x)",
      "url": "http://localhost:18011/api/v1/pipelines/catalog",
      "status_code": 404,
      "status_text": "Not Found",
      "success": false,
      "headers": {
        "date": "Fri, 26 Sep 2025 14:44:55 GMT",
        "server": "uvicorn",
        "content-length": "211",
        "content-type": "application/json"
      },
      "response_size": 211,
      "json_response": {
        "error": {
          "code": "PIPELINE_NOT_FOUND",
          "message": "Pipeline 'catalog' not found",
          "detail": "Pipeline 'catalog' not found",
          "hint": "Run 'videoannotator pipelines --detailed'"
        },
        "detail": "Pipeline 'catalog' not found"
      }
    },
    {
      "endpoint": "/api/v1/pipelines/schema",
      "description": "Pipeline schemas (v1.2.x)",
      "url": "http://localhost:18011/api/v1/pipelines/schema",
      "status_code": 404,
      "status_text": "Not Found",
      "success": false,
      "headers": {
        "date": "Fri, 26 Sep 2025 14:44:57 GMT",
        "server": "uvicorn",
        "content-length": "208",
        "content-type": "application/json"
      },
      "response_size": 208,
      "json_response": {
        "error": {
          "code": "PIPELINE_NOT_FOUND",
          "message": "Pipeline 'schema' not found",
          "detail": "Pipeline 'schema' not found",
          "hint": "Run 'videoannotator pipelines --detailed'"
        },
        "detail": "Pipeline 'schema' not found"
      }
    },
    {
      "endpoint": "/api/v1/jobs",
      "description": "Job list",
      "url": "http://localhost:18011/api/v1/jobs",
      "status_code": 500,
      "status_text": "Internal Server Error",
      "success": false,
      "headers": {
        "date": "Fri, 26 Sep 2025 14:44:59 GMT",
        "server": "uvicorn",
        "content-length": "281",
        "content-type": "application/json"
      },
      "response_size": 281,
      "json_response": {
        "error": {
          "code": "HTTP_ERROR",
          "message": "Failed to list jobs: attempted relative import beyond top-level package",
          "detail": "Failed to list jobs: attempted relative import beyond top-level package"
        },
        "detail": "Failed to list jobs: attempted relative import beyond top-level package"
      }
    },
    {
      "endpoint": "/api/v1/debug/token-info",
      "description": "Token information",
      "url": "http://localhost:18011/api/v1/debug/token-info",
      "status_code": 200,
      "status_text": "OK",
      "success": true,
      "headers": {
        "date": "Fri, 26 Sep 2025 14:45:01 GMT",
        "server": "uvicorn",
        "content-length": "434",
        "content-type": "application/json"
      },
      "response_size": 434,
      "json_response": {
        "token": {
          "valid": true,
          "user_id": "test-user-123",
          "username": "test_user",
          "permissions": [
            "job:submit",
            "job:read",
            "job:delete",
            "pipeline:read",
            "system:read"
          ],
          "created_at": "2025-01-23T10:00:00Z",
          "expires_at": null,
          "rate_limit": {
            "requests_per_minute": 100,
            "requests_per_hour": 1000,
            "remaining_this_minute": 87,
            "remaining_this_hour": 945,
            "reset_at": "2025-09-26T14:46:00"
          }
        },
        "session": {
          "first_request": null,
          "last_request": null,
          "total_requests": 0
        }
      }
    },
    {
      "endpoint": "/api/v1/debug/server-info",
      "description": "Debug server info",
      "url": "http://localhost:18011/api/v1/debug/server-info",
      "status_code": 200,
      "status_text": "OK",
      "success": true,
      "headers": {
        "date": "Fri, 26 Sep 2025 14:45:04 GMT",
        "server": "uvicorn",
        "content-length": "1102",
        "content-type": "application/json"
      },
      "response_size": 1102,
      "json_response": {
        "server": {
          "version": "1.2.0",
          "videoannotator_version": "1.2.2",
          "environment": "development",
          "start_time": "2025-09-26T10:50:52.723586",
          "uptime_seconds": 14051,
          "debug_mode": true,
          "api_base_path": "/api/v1"
        },
        "database": {
          "backend": "sqlite",
          "path": "/workspaces/VideoAnnotator/videoannotator.db",
          "connection_status": "healthy",
          "total_jobs": 2,
          "active_connections": 1,
          "size_mb": 0.05
        },
        "pipelines": {
          "initialized": [
            "person_tracking",
            "face_analysis",
            "scene_detection",
            "audio_processing"
          ],
          "available": [
            "person_tracking",
            "face_analysis",
            "scene_detection",
            "audio_processing"
          ],
          "loading_errors": [],
          "total_available": 4,
          "initialization_time": "2.3 seconds"
        },
        "system": {
          "platform": "Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35",
          "python_version": "3.12.11",
          "cpu_count": 24,
          "cpu_usage_percent": 0.8,
          "memory_total_gb": 15.5,
          "memory_used_gb": 8.64,
          "memory_usage_percent": 57.8,
          "gpu": {
            "available": true,
            "device_count": 1,
            "current_device": 0,
            "device_name": "NVIDIA GeForce GTX 1060 6GB",
            "memory_allocated_gb": 0.0,
            "memory_reserved_gb": 0.0
          }
        },
        "request_stats": {
          "total_requests": 0,
          "requests_last_hour": 0,
          "average_response_time_ms": 0
        }
      }
    },
    {
      "endpoint": "/api/v1/debug/routes",
      "description": "Available routes (debug)",
      "url": "http://localhost:18011/api/v1/debug/routes",
      "status_code": 404,
      "status_text": "Not Found",
      "success": false,
      "headers": {
        "date": "Fri, 26 Sep 2025 14:45:06 GMT",
        "server": "uvicorn",
        "content-length": "22",
        "content-type": "application/json"
      },
      "response_size": 22,
      "json_response": {
        "detail": "Not Found"
      }
    },
    {
      "endpoint": "/api/v1/debug/endpoints",
      "description": "Available endpoints (debug)",
      "url": "http://localhost:18011/api/v1/debug/endpoints",
      "status_code": 404,
      "status_text": "Not Found",
      "success": false,
      "headers": {
        "date": "Fri, 26 Sep 2025 14:45:08 GMT",
        "server": "uvicorn",
        "content-length": "22",
        "content-type": "application/json"
      },
      "response_size": 22,
      "json_response": {
        "detail": "Not Found"
      }
    },
    {
      "endpoint": "/api/system/info",
      "description": "Alternative system info path",
      "url": "http://localhost:18011/api/system/info",
      "status_code": 404,
      "status_text": "Not Found",
      "success": false,
      "headers": {
        "date": "Fri, 26 Sep 2025 14:45:10 GMT",
        "server": "uvicorn",
        "content-length": "22",
        "content-type": "application/json"
      },
      "response_size": 22,
      "json_response": {
        "detail": "Not Found"
      }
    },
    {
      "endpoint": "/system/info",
      "description": "Root system info path",
      "url": "http://localhost:18011/system/info",
      "status_code": 404,
      "status_text": "Not Found",
      "success": false,
      "headers": {
        "date": "Fri, 26 Sep 2025 14:45:12 GMT",
        "server": "uvicorn",
        "content-length": "22",
        "content-type": "application/json"
      },
      "response_size": 22,
      "json_response": {
        "detail": "Not Found"
      }
    },
    {
      "endpoint": "/version",
      "description": "Root version path",
      "url": "http://localhost:18011/version",
      "status_code": 404,
      "status_text": "Not Found",
      "success": false,
      "headers": {
        "date": "Fri, 26 Sep 2025 14:45:14 GMT",
        "server": "uvicorn",
        "content-length": "22",
        "content-type": "application/json"
      },
      "response_size": 22,
      "json_response": {
        "detail": "Not Found"
      }
    },
    {
      "endpoint": "/api/version",
      "description": "API version path",
      "url": "http://localhost:18011/api/version",
      "status_code": 404,
      "status_text": "Not Found",
      "success": false,
      "headers": {
        "date": "Fri, 26 Sep 2025 14:45:16 GMT",
        "server": "uvicorn",
        "content-length": "22",
        "content-type": "application/json"
      },
      "response_size": 22,
      "json_response": {
        "detail": "Not Found"
      }
    }
  ]
}